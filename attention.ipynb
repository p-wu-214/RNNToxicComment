{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "\n",
    "import os, sys\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding, \\\n",
    "    Bidirectional, RepeatVector, Concatenate, Activation, Dot, Lambda\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do softmax over the time axis\n",
    "# expected shape is N x T x D\n",
    "# where N = # of samples, T = sequence length (Time), D = vector dimensionality\n",
    "def softmax_over_time(x):\n",
    "    assert(K.ndim(x) > 2)\n",
    "    # maximum value in a tensor\n",
    "    # formula: e^(x-max(x))/sum(e^(x-max(x)))\n",
    "    e = K.exp(x-K.max(x, axis=1, keepdims=True))\n",
    "    s = K.sum(e, axis=1, keepdims=True)\n",
    "    return e/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 1\n",
    "ENCODER_LSTM_HIDDEN_DIM = 256\n",
    "DECODER_LSTM_HIDDEN_DIM = 256\n",
    "NUM_SAMPLES = 10\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "output_texts = []\n",
    "output_texts_inputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples: 10\n"
     ]
    }
   ],
   "source": [
    "t = 0\n",
    "for line in open('data/fra.txt'):\n",
    "    t +=1\n",
    "    if t > NUM_SAMPLES:\n",
    "        break\n",
    "    if '\\t' not in line:\n",
    "        continue\n",
    "    input_text, translation, _ = line.rstrip().split('\\t')\n",
    "    \n",
    "    output_text = translation + '<eos>'\n",
    "    output_text_input = '<sos> ' + translation\n",
    "    \n",
    "    input_texts.append(input_text)\n",
    "    output_texts.append(output_text)\n",
    "    output_texts_inputs.append(output_text_input)\n",
    "print ('num samples:', len(input_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_inputs = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer_inputs.fit_on_texts(input_texts)\n",
    "input_sequences = tokenizer_inputs.texts_to_sequences(input_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found %s unique input tokens 8\n"
     ]
    }
   ],
   "source": [
    "word2idx_inputs = tokenizer_inputs.word_index\n",
    "print('Found %s unique input tokens', len(word2idx_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max len input: 1\n"
     ]
    }
   ],
   "source": [
    "max_len_input = max(len(s) for s in input_sequences)\n",
    "print('max len input:', max_len_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_outputs = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\n",
    "tokenizer_outputs.fit_on_texts(output_texts)\n",
    "output_sequences = tokenizer_outputs.texts_to_sequences(output_texts)\n",
    "output_sequences_inputs = tokenizer_outputs.texts_to_sequences(output_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found %s unique input tokens 15\n"
     ]
    }
   ],
   "source": [
    "word2idx_outputs = tokenizer_outputs.word_index\n",
    "print('Found %s unique input tokens', len(word2idx_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max len output is: 3\n"
     ]
    }
   ],
   "source": [
    "max_len_output = max(len(s) for s in output_sequences)\n",
    "print('Max len output is:', max_len_output)\n",
    "max_len_output = len(word2idx_outputs) + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sequences[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_inputs.shape: (10, 1)\n",
      "encoder_inputs[0] [3]\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = pad_sequences(input_sequences, maxlen=max_len_input)\n",
    "print('encoder_inputs.shape:', encoder_inputs.shape)\n",
    "print('encoder_inputs[0]', encoder_inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_inputs.shape: (10, 25)\n",
      "decoder_inputs[0] [2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "decoder_inputs = pad_sequences(output_sequences_inputs, maxlen=max_len_output, padding='post')\n",
    "print('decoder_inputs.shape:', decoder_inputs.shape)\n",
    "print('decoder_inputs[0]', decoder_inputs[0])\n",
    "\n",
    "decoder_outputs = pad_sequences(output_sequences, maxlen=max_len_output, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found %s word vectors 400000\n"
     ]
    }
   ],
   "source": [
    "word2vec = {}\n",
    "with open(os.path.join('word_embedding/glove.6B.%sd.txt' %EMBEDDING_DIM)) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        word2vec[word] = vec\n",
    "print('found %s word vectors', len(word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word2idx_inputs.items():\n",
    "    if i < MAX_NUM_WORDS:\n",
    "        embedding_vector = word2vec.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(\n",
    "    num_words,\n",
    "    EMBEDDING_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=max_len_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 25)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_outputs_one_hot = np.zeros((\n",
    "    len(input_texts),\n",
    "    max_len_output,\n",
    "    max_len_output\n",
    "))\n",
    "\n",
    "for i, d in enumerate(decoder_outputs):\n",
    "    for t, word in enumerate(d):\n",
    "        decoder_outputs_one_hot[i, t, word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice_2:0\", shape=(None, 1, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "##### build the model #####\n",
    "\n",
    "# Set up the encoder - simple!\n",
    "encoder_inputs_placeholder = Input(shape=(max_len_input,))\n",
    "x = embedding_layer(encoder_inputs_placeholder)\n",
    "encoder = Bidirectional(LSTM(\n",
    "  ENCODER_LSTM_HIDDEN_DIM,\n",
    "  return_sequences=True,\n",
    "  # dropout=0.5 # dropout not available on gpu\n",
    "))\n",
    "encoder_outputs = encoder(x)\n",
    "\n",
    "\n",
    "# Set up the decoder - not so simple\n",
    "decoder_inputs_placeholder = Input(shape=(max_len_output,))\n",
    "\n",
    "# this word embedding will not use pre-trained vectors\n",
    "# although you could\n",
    "decoder_embedding = Embedding(max_len_output, EMBEDDING_DIM)\n",
    "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Attention #########\n",
    "# Attention layers need to be global because\n",
    "# they will be repeated Ty times at the decoder\n",
    "# repeats the input n times (n being the parameter)\n",
    "\n",
    "# eg: input=(x, 23) -> RepeatVector -> output=(x, n, 23)\n",
    "# max_len_input = Tx\n",
    "attn_repeat_layer = RepeatVector(max_len_input)\n",
    "# concatenates tensors based on the axis chose\n",
    "# note here the axis is the last one (-1)\n",
    "attn_concat_layer = Concatenate(axis=-1)\n",
    "attn_dense1 = Dense(10, activation='tanh')\n",
    "attn_dense2 = Dense(1, activation=softmax_over_time)\n",
    "attn_dot = Dot(axes=1) # to perform the weighted sum of alpha[t] * h[t]\n",
    "\n",
    "# Generating one step of Ty requires sum over all Tx\n",
    "def one_step_attention(h, sty_1):\n",
    "    # this is s(ty-1) or the previous decoder's state\n",
    "    # we are copying s(ty-1) Tx times because we will calculate\n",
    "    # the sum of O(ty, tx) over Tx and O(ty, tx) is a obtained from NN of s(ty-1) and a(tx)\n",
    "    sty_1 = attn_repeat_layer(sty_1)\n",
    "\n",
    "    # note h has output shape of [Tx, LATENT_DIM*2] due to bidirectional\n",
    "    # lstm and sty_1 is [Tx, Decoder_hidden_units + LATENT_DIM*2]\n",
    "    x = attn_concat_layer([h, sty_1])\n",
    "\n",
    "    x = attn_dense1(x)\n",
    "\n",
    "    alphas = attn_dense2(x)\n",
    "\n",
    "    context = attn_dot([alphas, h])\n",
    "    # Tells the decoder which hidden state (h) we care about the most\n",
    "    # context = sum(attention_weights * hidden state)\n",
    "    # attention_weights = NeuralNetwork([St-1, h])\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the rest of the decoder (after attention)\n",
    "decoder_lstm = LSTM(DECODER_LSTM_HIDDEN_DIM, return_state=True)\n",
    "decoder_dense = Dense(max_len_output, activation='softmax')\n",
    "\n",
    "# Unique name for s and c\n",
    "initial_s = Input(shape=(DECODER_LSTM_HIDDEN_DIM,), name='s0')\n",
    "initial_c = Input(shape=(DECODER_LSTM_HIDDEN_DIM,), name='c0')\n",
    "context_last_word_concat_layer = Concatenate(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = initial_s\n",
    "c = initial_c\n",
    "\n",
    "outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(max_len_output): # Ty times\n",
    "    # get the context using attention\n",
    "    context = one_step_attention(encoder_outputs, s)\n",
    "\n",
    "    # Lambda wraps arbitrary expression as a Layber object\n",
    "    # lambda is like arrow function in JS a small function\n",
    "    # numpy when doing x[:, 1:2] will retrieve all elements in column\n",
    "    # two and each element will be in its own array while x[:, 1]\n",
    "    # will retrieve all elements in column two and store in one array\n",
    "    selector = Lambda(lambda x: x[:, t:t+1])\n",
    "    xt = selector(decoder_inputs_x)\n",
    "  \n",
    "    # combine \n",
    "    decoder_lstm_input = context_last_word_concat_layer([context, xt])\n",
    "#     print(decoder_lstm_input.shape)\n",
    "\n",
    "    # pass the combined [context, last word] into the LSTM\n",
    "    # along with [s, c]\n",
    "    # get the new [s, c] and output\n",
    "    o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[s, c])\n",
    "\n",
    "    # final dense layer to get next word prediction\n",
    "    decoder_outputs = decoder_dense(o)\n",
    "    outputs.append(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before transpose 25\n",
      "After transpose (None, 25, 25)\n"
     ]
    }
   ],
   "source": [
    "def stack_and_transpose(x):\n",
    "    # x is a list of length T, each element is a batch_size x output_vocab_size tensor\n",
    "    x = K.stack(x) # is now T x batch_size x output_vocab_size tensor\n",
    "    x = K.permute_dimensions(x, pattern=(1, 0, 2)) # is now batch_size x T x output_vocab_size\n",
    "    return x\n",
    "\n",
    "# make it a layer\n",
    "stacker = Lambda(stack_and_transpose)\n",
    "print('Before transpose', len(outputs))\n",
    "outputs = stacker(outputs)\n",
    "print('After transpose', outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'strided_slice_6:0' shape=(None, 25, 25) dtype=float32>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    inputs=[\n",
    "        encoder_inputs_placeholder,\n",
    "        decoder_inputs_placeholder,\n",
    "        initial_s, \n",
    "        initial_c,\n",
    "    ],\n",
    "    outputs=outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    # both are of shape N x T x K\n",
    "    mask = K.cast(y_true > 0, dtype='float32')\n",
    "    out = mask * y_true * K.log(y_pred)\n",
    "    return -K.sum(out) / K.sum(mask)\n",
    "\n",
    "\n",
    "def acc(y_true, y_pred):\n",
    "    # both are of shape N x T x K\n",
    "    targ = K.argmax(y_true, axis=-1)\n",
    "    pred = K.argmax(y_pred, axis=-1)\n",
    "    correct = K.cast(K.equal(targ, pred), dtype='float32')\n",
    "\n",
    "    # 0 is padding, don't include those\n",
    "    mask = K.cast(K.greater(targ, 0), dtype='float32')\n",
    "    n_correct = K.sum(mask * correct)\n",
    "    n_total = K.sum(mask)\n",
    "    return n_correct / n_total\n",
    "\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss=custom_loss, metrics=[acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_inputs[9,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "z = np.zeros((len(encoder_inputs), LATENT_DIM_DECODER)) # initial [s, c]\n",
    "r = model.fit(\n",
    "  [encoder_inputs, decoder_inputs, z, z], decoder_targets_one_hot,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  epochs=EPOCHS,\n",
    "  validation_split=0.2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
